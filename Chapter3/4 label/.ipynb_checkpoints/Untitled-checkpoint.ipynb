{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd3b00-743a-418d-9226-3f2741c5ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import sort\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "import scipy.cluster\n",
    "from numpy import absolute, mean, sort, std\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import re\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "from sklearn import datasets, metrics, preprocessing, model_selection\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold,RepeatedKFold, cross_val_score, cross_validate, cross_val_predict, GridSearchCV, RandomizedSearchCV, validation_curve, learning_curve\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error, max_error\n",
    "\n",
    "import skopt\n",
    "from skopt import BayesSearchCV \n",
    "\n",
    "from missingpy import MissForest\n",
    "\n",
    "import shap\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier, StackingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "\n",
    "seed = 0\n",
    "\n",
    "dataset = pd.read_csv(\"4l_training_data.txt\", sep=\"\\t\")\n",
    "dataset = dataset.rename({'IPA_BP':'IPA_BP_annotation'}, axis=1)\n",
    "\n",
    "data = dataset.drop(\n",
    "    [\"Gene\", ], 1\n",
    ")  \n",
    "\n",
    "data[\"BPlabel_encoded\"] = data[\"BPlabel\"].map(\n",
    "    {\"most likely\": 1, \"probable\": 2, \"possible\": 3, \"least likely\": 4}\n",
    ")\n",
    "Y = data[\"BPlabel_encoded\"]\n",
    "\n",
    "natest = data.isnull().sum()\n",
    "natest.sort_values(inplace=True)\n",
    "\n",
    "percent_missing = data.isnull().sum() * 100 / len(data)\n",
    "missing_value_df = pd.DataFrame(\n",
    "    {\"column_name\": data.columns, \"percent_missing\": percent_missing}\n",
    ")\n",
    "missing_value_df.sort_values(\"percent_missing\", inplace=True)\n",
    "\n",
    "natest = natest.to_frame()\n",
    "missingdata = natest.join(missing_value_df)\n",
    "\n",
    "missingdata.to_csv(\"training_data_missingness.csv\")\n",
    "\n",
    "data_drop = data.drop(\n",
    "    [\"BPlabel\", \"BPlabel_encoded\",], 1\n",
    ") \n",
    "\n",
    "null_counts = data_drop.isnull().sum() / len(data_drop)\n",
    "\n",
    "selection = missing_value_df[missing_value_df[\"percent_missing\"] < 25.00]\n",
    "list(selection[\"column_name\"])\n",
    "\n",
    "dat = data[list(selection[\"column_name\"])]\n",
    "dat[\"Gene\"] = dataset[\"Gene\"]\n",
    "\n",
    "dt2 = dat\n",
    "dat = dat.set_index(\"Gene\")\n",
    "\n",
    "df = dt2\n",
    "df = df.set_index(\"Gene\")\n",
    "df[\"BPlabel_encoded\"] = df[\"BPlabel\"].map(\n",
    "    {\"most likely\": 1, \"probable\": 2, \"possible\": 3, \"least likely\": 4}\n",
    ")\n",
    "\n",
    "# removing remaining variant-level features strongly correlating with gene length:\n",
    "df = df.drop([\"BPlabel\", \"Gene_length\", \"CADD_RAW\", \"gwastrait\",'ExomiserScore',\n",
    " 'ppiscore_Exomiser','DNaseCluster_count', 'H3k4me1_count','CpGcount', 'EnhancerCount','H3k4me3_count', 'H3k27Ac_count'], 1, errors='ignore')\n",
    "\n",
    "X = MinMaxScaler().fit_transform(df)\n",
    "imputer = MissForest(random_state=seed)\n",
    "X = pd.DataFrame(imputer.fit_transform(X), index=df.index, columns=df.columns)\n",
    "\n",
    "Xcor = X\n",
    "Xcor = pd.DataFrame(data=Xcor, columns=X.columns)\n",
    "corr = Xcor.corr(method=\"spearman\").abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape),k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "print(\"Dropped features with > 0.85 correlation:\", to_drop)\n",
    "\n",
    "selected_columns = X.drop(X[to_drop], axis=1)\n",
    "\n",
    "dt_towrite = data[list(selected_columns)]\n",
    "dt_towrite[\"Gene\"] = dataset[\"Gene\"]\n",
    "dt_towrite[\"BPlabel\"] = dataset[\"BPlabel\"]\n",
    "dt_towrite = dt_towrite.drop([\"BPlabel_encoded\"], 1)\n",
    "\n",
    "dt_towrite.to_csv(\"training_cleaned.csv\", header=True, index=False)\n",
    "\n",
    "data = pd.read_csv(\"training_cleaned.csv\", header=0, sep=\",\")\n",
    "\n",
    "data[\"BPlabel_encoded\"] = data[\"BPlabel\"].map(\n",
    "    {\"most likely\": 1, \"probable\": 2, \"possible\": 3, \"least likely\": 4}\n",
    ")\n",
    "Y = data[\"BPlabel_encoded\"]\n",
    "data = data.drop([\"BPlabel\"], 1)\n",
    "\n",
    "df1 = data.iloc[:, :-1]\n",
    "print('Training data dimensions with all features:', df1.shape)\n",
    "\n",
    "df = df1.set_index(\"Gene\")\n",
    "\n",
    "imputer = MissForest(random_state=seed)\n",
    "X = pd.DataFrame(imputer.fit_transform(df), index=df.index, columns=df.columns)\n",
    "\n",
    "X.to_csv(r\"imputed_training_data.csv\", index=False)\n",
    "\n",
    "data = pd.read_csv(\"training_cleaned.csv\", header=0, sep=\",\")\n",
    "\n",
    "data[\"BPlabel_encoded\"] = data[\"BPlabel\"].map(\n",
    "    {\"most likely\": 1, \"probable\": 2, \"possible\": 3, \"least likely\": 4}\n",
    ")\n",
    "Y = data[\"BPlabel_encoded\"]\n",
    "data = data.drop([\"BPlabel\"], 1)\n",
    "\n",
    "X = pd.read_csv(\"imputed_training_data.csv\", header=0)\n",
    "X.columns = [\n",
    "    regex.sub(\"_\", col) if any(x in str(col) for x in set((\"[\", \"]\", \"<\"))) else col\n",
    "    for col in X.columns.values\n",
    "]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "xgbr = xgboost.XGBClassifier(random_state=seed, objective='reg:squarederror', verbosity = 0, eval_metric='mlogloss') \n",
    "xgbr_params = {\n",
    "    'max_depth':  (1, 4), \n",
    "    'learning_rate': (0.01, 0.2, 'log-uniform'),  \n",
    "    'n_estimators':  (10, 50), \n",
    "    'reg_alpha':  (1, 10, 'log-uniform'), \n",
    "    'reg_lambda':  (1, 10, 'log-uniform')} \n",
    "\n",
    "lgbm = LGBMClassifier(random_state=seed)\n",
    "lgbm_params = {\n",
    "    \"max_depth\": (1, 4),\n",
    "    \"learning_rate\": (0.01, 0.2, \"log-uniform\"),\n",
    "    \"n_estimators\": (10, 50),\n",
    "    \"reg_alpha\": (1, 10, \"log-uniform\"),\n",
    "    \"reg_lambda\": (1, 10, \"log-uniform\"),\n",
    "}\n",
    "\n",
    "catboost = CatBoostClassifier(random_seed=seed, verbose=False)\n",
    "cat_params = {\n",
    "     \"iterations\": (10, 50),\n",
    "     'learning_rate': (0.01, 0.2, 'log-uniform'), \n",
    "     'depth':  (1, 4), \n",
    "}\n",
    "\n",
    "\n",
    "gbr = GradientBoostingClassifier(random_state=seed)\n",
    "gbr_params = {\n",
    "    'learning_rate': (0.01, 0.2),\n",
    "    'max_depth': (1, 4),\n",
    "    \"max_features\":[\"log2\",\"sqrt\", \"auto\"],\n",
    "    \"criterion\": [\"friedman_mse\", \"mse\", \"mae\"],\n",
    "    'n_estimators': (10, 50)\n",
    "    }\n",
    "\n",
    "rfr = RandomForestClassifier(random_state=seed)\n",
    "rfr_params={'n_estimators': (10, 50), \n",
    "             'max_features': ['auto', 'sqrt', 'log2'],\n",
    "             'max_depth' : (1, 4),\n",
    "             'criterion' :['mse', 'mae']} \n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=seed)\n",
    "dt_params= {\"criterion\": [\"mse\", \"mae\"],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'max_depth' : (1, 4)}\n",
    "\n",
    "extra = ExtraTreesClassifier(random_state=seed)\n",
    "extra_params ={'n_estimators': (10, 50), \n",
    "             'max_features': ['auto', 'sqrt', 'log2'],\n",
    "             'max_depth' : (1, 4),\n",
    "             'criterion' :['mse', 'mae']}\n",
    "\n",
    "\n",
    "inner_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=seed)\n",
    "outer_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=seed)\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append(('XGBR', BayesSearchCV(xgbr, xgbr_params, cv=inner_cv,iid=False,n_jobs=1, random_state=seed))) \n",
    "models.append((\"LGBM\", BayesSearchCV(lgbm, lgbm_params, cv=inner_cv, iid=False, n_jobs=1, random_state=seed)))\n",
    "models.append((\"CB\", BayesSearchCV(catboost, cat_params, cv=inner_cv, iid=False, n_jobs=1, random_state=seed)))\n",
    "models.append(('GBR', BayesSearchCV(gbr, gbr_params, cv=inner_cv,iid=False, n_jobs=1, random_state=seed)))\n",
    "models.append(('RFR', BayesSearchCV(rfr, rfr_params, cv=inner_cv,iid=False, n_jobs=1, random_state=seed)))\n",
    "models.append(('DT', BayesSearchCV(dt, dt_params, cv=inner_cv, iid=False, n_jobs=1, random_state=seed)))\n",
    "models.append(('ExtraTrees', BayesSearchCV(extra, extra_params, cv=inner_cv, iid=False, n_jobs=1, random_state=seed)))\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "medians =[]\n",
    "scoring = ['accuracy', 'balanced_accuracy', 'f1_weighted', \n",
    "          'precision_weighted','recall_weighted']\n",
    "\n",
    "models_list_balancedac = []\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    nested_cv_results = model_selection.cross_validate(model, X , Y, cv=outer_cv, scoring=scoring, error_score=\"raise\")\n",
    "    nested_cv_results2 = model_selection.cross_val_score(model, X , Y, cv=outer_cv, scoring='balanced_accuracy', error_score=\"raise\")\n",
    "    results.append(nested_cv_results2)\n",
    "    names.append(name)\n",
    "    print(name, 'Nested CV results for all scores:', '\\n', nested_cv_results, '\\n')\n",
    "    print(name, 'Accuracy Nested CV Average', np.mean(nested_cv_results['test_accuracy']))\n",
    "    print(name, 'Balanced Accuracy Nested CV Average', np.mean(nested_cv_results['test_balanced_accuracy'] ))\n",
    "    print(name, 'F1 Nested CV Average', np.mean(nested_cv_results['test_f1_weighted'] ))\n",
    "    print(name, 'Precision Nested CV Average', np.mean(nested_cv_results['test_precision_weighted'] ))\n",
    "    print(name, 'Recall Nested CV Average', np.mean(nested_cv_results['test_recall_weighted'] ))\n",
    "    model.fit(X, Y)\n",
    "    print('\\n')\n",
    "    print(\"Best Parameters: \\n{}\\n\".format(model.best_params_))\n",
    "    print(\"Best Estimator:\", model.best_estimator_)\n",
    "    best_model = model.best_estimator_\n",
    "    print('\\n')\n",
    "    print('Non-nested CV Results:')\n",
    "    best_model.fit(X_train, Y_train)\n",
    "    y_pred_train = best_model.predict(X_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(name, 'Train accuracy:', accuracy_score(Y_train, y_pred_train), 'Test accuracy:', accuracy_score(Y_test, y_pred))\n",
    "    print(name, 'Train balanced accuracy:', balanced_accuracy_score(Y_train, y_pred_train), 'Test balanced accuracy:', balanced_accuracy_score(Y_test, y_pred))\n",
    "    print(name, 'Train F1', f1_score(Y_train, y_pred_train, average='weighted'), 'Test F1:', f1_score(Y_test, y_pred, average='weighted'))\n",
    "    print(name, 'Train recall:', recall_score(Y_train, y_pred_train, average='weighted'),'Test recall:', recall_score(Y_test, y_pred,average='weighted'))\n",
    "    print(name, 'Train precision:', precision_score(Y_train, y_pred_train,average='weighted'), 'Test precision:', precision_score(Y_test, y_pred,average='weighted'))\n",
    "    print('\\n')\n",
    "    best_model.fit(X, Y)\n",
    "    median_balancedac = np.median(nested_cv_results['test_balanced_accuracy'])\n",
    "    models_list_balancedac.append((best_model, median_balancedac))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9865d-6d67-4564-8279-0315c661593e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
